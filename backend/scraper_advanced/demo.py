#!/usr/bin/env python3
"""
Production-Grade News Scraper Demo
Showcase of advanced anti-bot evasion features
"""

import asyncio
import logging
import time
from scraper_advanced.scraper import AdvancedNewsScraper

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

async def demo():
    """Comprehensive scraper demonstration"""

    print("ğŸš€ Advanced News Scraper Demo")
    print("=" * 50)

    scraper = AdvancedNewsScraper()

    try:
        # 1. Health Check
        print("\nğŸ“Š Health Check:")
        health = await scraper.health_check()
        print(f"   Status: {health['overall']}")
        for component, status in health['components'].items():
            print(f"   {component}: {status['status']}")

        # 2. Test Scrape (Safe test URLs)
        print("\nğŸ§ª Testing Scraper Components:")

        # Test with a simple, accessible site first
        test_url = "https://httpbin.org/html"  # Simple test endpoint
        print(f"   Testing with: {test_url}")

        start_time = time.time()
        result = await scraper.scrape_article(test_url, "test")
        elapsed = time.time() - start_time

        print(f"   Response Time: {elapsed:.2f}s")
        print(f"   Content Length: {len(result.get('content', ''))} chars")
        print(f"   Extraction Method: {result.get('extraction_method', 'unknown')}")

        # 3. Show Statistics
        print("\nğŸ“ˆ Scraper Statistics:")
        stats = await scraper.get_stats()
        print(f"   Runtime: {stats['runtime_seconds']:.1f}s")
        print(f"   Requests Made: {stats['requests_made']}")
        print(f"   Success Rate: {stats['success_rate']:.1f}%")

        if 'proxy_stats' in stats and stats['proxy_stats']['total_proxies'] > 0:
            proxy = stats['proxy_stats']
            print(f"   Proxies: {proxy['healthy_proxies']}/{proxy['total_proxies']} healthy")

        # 4. Feature Demonstration
        print("\nğŸ¯ Key Features Demonstrated:")
        print("   âœ… curl_cffi with Chrome TLS impersonation")
        print("   âœ… Browser fingerprint spoofing")
        print("   âœ… Human-like behavior delays")
        print("   âœ… Residential proxy rotation")
        print("   âœ… Cloudflare bypass (FlareSolverr ready)")
        print("   âœ… Undetected Chrome fallback")
        print("   âœ… Smart retry with exponential backoff")
        print("   âœ… Clean article extraction (trafilatura)")
        print("   âœ… Session persistence")
        print("   âœ… Docker-ready with sidecar services")

        # 5. Usage Examples
        print("\nğŸ’¡ Usage Examples:")
        print("   # Single article")
        print("   python -m scraper_advanced.cli scrape https://newsmax.com/article")
        print()
        print("   # Multiple articles")
        print("   python -m scraper_advanced.cli scrape-site newsmax --urls url1 url2")
        print()
        print("   # Docker deployment")
        print("   docker-compose up scraper flaresolverr")
        print()
        print("   # Health monitoring")
        print("   python -m scraper_advanced.cli health")

        print("\nğŸ‰ Demo completed successfully!")
        print("\nâš ï¸  Note: For production use with Newsmax/Breitbart/OANN,")
        print("         configure residential proxies in config.yaml")

    except Exception as e:
        print(f"\nâŒ Demo failed: {str(e)}")
        logging.exception("Demo error")

    finally:
        await scraper.cleanup()

def main():
    """Main demo entry point"""
    print("Advanced News Scraper - Production Demo")
    print("Specialized for Newsmax, Breitbart, and OANN")
    print()

    try:
        asyncio.run(demo())
    except KeyboardInterrupt:
        print("\nğŸ›‘ Demo interrupted by user")
    except Exception as e:
        print(f"\nğŸ’¥ Demo crashed: {str(e)}")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())
